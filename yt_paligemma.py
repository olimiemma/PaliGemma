# -*- coding: utf-8 -*-
"""YT PaliGemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rK74ACchwi7o7Nwo9X_pSQQpzaJITWGi

# Using PaliGemma with ðŸ¤— transformers

Modified from the hugginface notebook

PaliGemma is a new vision language model released by Google. In this notebook, we will see how to use ðŸ¤— transformers for PaliGemma inference.
First, install below libraries with update flag as we need to use the latest version of ðŸ¤— transformers along with others.
"""

!pip install -q -U accelerate bitsandbytes git+https://github.com/huggingface/transformers.git

"""PaliGemma requires users to accept Gemma license, so make sure to go to [the repository]() and ask for access. If you have previously accepted Gemma license, you will have access to this model as well. Once you have the access, login to Hugging Face Hub using `notebook_login()` and pass your access token by running the cell below."""

# from huggingface_hub import notebook_login

# notebook_login()


import os
from google.colab import userdata

os.environ["HF_TOKEN"] = userdata.get('HF_TOKEN')  # having the hugging face token in secrets

import torch
import numpy as np
from PIL import Image
import requests

input_text = "What color is the flower that bee is standing on?"
img_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/bee.JPG?download=true"
input_image = Image.open(requests.get(img_url, stream=True).raw) #outting it an an image

"""The image looks like below.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/bee.JPG?download=true)

You can load PaliGemma model and processor like below.
"""

from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor #puts image thru encoder
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_id = "google/paligemma-3b-mix-224" #change to go to higher resolution models
model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16)
processor = PaliGemmaProcessor.from_pretrained(model_id)

"""The processor preprocesses both the image and text, so we will pass them."""

inputs = processor(text=input_text, images=input_image,
                  padding="longest", do_convert_rgb=True, return_tensors="pt").to("cuda") #coverts it to rgb. and retunrs it as pytorch sensors
model.to(device)
inputs = inputs.to(dtype=model.dtype)

"""We can pass in our preprocessed inputs."""

with torch.no_grad(): # This processor acts like our tokenizer. Converts images to tokens, ext to token and putting them into context window.
  output = model.generate(**inputs, max_length=496)

print(processor.decode(output[0], skip_special_tokens=True))

"""## Load model in 4-bit

You can also load model in 4-bit and 8-bit, which offers memory gains during inference.
First, initialize the `BitsAndBytesConfig`.
"""

from transformers import BitsAndBytesConfig #makes inference faster when laoded as 4bit modle
import torch
nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)

"""We will now reload the model but pass in above object as `quantization_config`."""

from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor
import torch

device="cuda"
model_id = "google/paligemma-3b-mix-224"
model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,
                                                          quantization_config=nf4_config, device_map={"":0}) #tellinh ti nwe use 4bit
processor = PaliGemmaProcessor.from_pretrained(model_id)

with torch.no_grad():
  output = model.generate(**inputs, max_length=496)

print(processor.decode(output[0], skip_special_tokens=True))

def query_paligemma(text, input_image):
    input_text = text
    input_image = input_image #Image.open(requests.get(img_url, stream=True).raw)

    inputs = processor(text=input_text, images=input_image,
                  padding="longest", do_convert_rgb=True, return_tensors="pt").to("cuda")

    inputs = inputs.to(dtype=model.dtype)

    with torch.no_grad():
        output = model.generate(**inputs, max_length=496)

    print(processor.decode(output[0], skip_special_tokens=True))

img_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/bee.JPG?download=true"


img_path = "/content/test_01.png"

input_image = Image.open(img_path).convert("RGB")
input_image

query_paligemma("what is funny about this?", input_image)

img_path = "/content/test_02.png"

input_image = Image.open(img_path).convert("RGB")
input_image

query_paligemma("what company makes these burgers?", input_image)

